{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd  \n",
    "import requests                                                                     # Importing packages from my Python venv \n",
    "from bs4 import BeautifulSoup \n",
    "import sqlite3\n",
    "import locale\n",
    "import altair as alt\n",
    "from vega_datasets import data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Scraping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp500_url = 'http://en.wikipedia.org/wiki/List_of_S%26P_500_companies'              # Assigning link to 'sp500_url'\n",
    "response = requests.get(sp500_url)                                                  # Using request to get access to 'sp500_url'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if response.status_code ==200:\n",
    "    print('Request successful')\n",
    "else:                                                                              # Checking status = 200 code using 'if' & 'else'\n",
    "    print('Request not successful')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "table = soup.find ('table')                                       # Using Beautifulsoup() to find table within my url in 'response'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_table = pd.read_html(str(table))[0]            # Using pandas to read link from table and assigning a value to a variable 'data_table'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Resulting DataFrame:\")\n",
    "\n",
    "print(data_table.head(503))      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Formatting .tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = data_table ['Symbol'].tolist()                                                              # Assigning 'tickers' to only the 'Symbol' of the 'data_table' and convert into a list()\n",
    "tickers_list = [ticker.replace('BF.B', 'BF-B').replace('BRK.B', 'BRK-B') for ticker in tickers]       # Replacing two incorrect Value 'tickers' within a variable 'tickers_list'\n",
    "\n",
    "print('Tickers list')\n",
    "print(tickers_list) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sectors_list = data_table ['GICS Sector'].tolist()       # Convert 'GICS Sector' into a list() within 'data_table' assigned to 'sectors_list'\n",
    "\n",
    "print('Sectors list')\n",
    "print(sectors_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks_list = data_table ['Security'].tolist()         # Convert 'Security' into a list() within 'data_table' assigned to 'stocks_list'\n",
    "\n",
    "print('Stocks list')\n",
    "print(stocks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Industry_list = data_table ['GICS Sub-Industry'].tolist()                     # Convert 'GICS Sub-Industry' into a list() within 'data_table' assigned to 'Industry_list'\n",
    "\n",
    "print('Industry list')\n",
    "print(Industry_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Location_list = data_table ['Headquarters Location'].tolist()                                  # Convert 'Headquarters Location' into a list() within 'data_table' assigned to 'Location_list'\n",
    "\n",
    "print('Headquarters Location list')\n",
    "print(Location_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrame Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Sp500_data_table_columns = pd.DataFrame({\n",
    "    'Tickers': tickers_list,\n",
    "    'Stocks': stocks_list,                                                  # Adding my variables ('tickers_list','stocks_list, and 'sectors_list') to a new Dataframe 'Sp500_columns' using pandas & {}\n",
    "    'Sectors': sectors_list,\n",
    "    'Industry': Industry_list,\n",
    "    'Location': Location_list,}) \n",
    "\n",
    "print(Sp500_data_table_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Data From Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers_volume = yf.download(tickers_list, period=\"1y\", interval=\"1d\")[['Volume']]    # Using 'yf' to download all the info in the 'tickers_list' between those dates and assigning it to 'tickers_price'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate The Volume For Each Ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_volume = tickers_volume['Volume'].resample('D').sum()                                                # Calculate the daily volume for each ticker\n",
    "\n",
    "weekly_average_volume = tickers_volume['Volume'].resample('W').mean()                               # Calculate the daily average volume for each ticker\n",
    "\n",
    "yearly_average_volume = tickers_volume['Volume'].resample('Y').mean()                               # Calculate the yearly average volume for each ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Daily Volume for Each Ticker:\")\n",
    "\n",
    "daily_volume.sort_values(by='Date',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Weekly Average Volume for Each Ticker:\")\n",
    "\n",
    "weekly_average_volume.sort_values(by='Date',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nYearly Average Volume for Each Ticker:\")\n",
    "\n",
    "print(yearly_average_volume)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Scraping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "market_cap_url = 'https://www.liberatedstocktrader.com/sp-500-companies/'            \n",
    "response_2 = requests.get(market_cap_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if response_2.status_code == 200 :\n",
    " print('Request successful')                                                                        # Checking status = 200 code using 'if' & 'else'\n",
    "else: print('Request not successful')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup_2 = BeautifulSoup(response_2.content, 'html.parser')                                        # Using Beautifulsoup() to find table within my url in 'response_2'\n",
    "market_table = soup_2.find_all ('table')                                                         # Using 'find_all' to read all the tables on the web\n",
    "Sp500_columns = pd.read_html(str(market_table[0]))[0]                                            # Using pd() to read the first table on the web"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Formatting (Market Cap) & Function Creation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sp500_columns.iloc[0] = ['Tickers', 'Names', 'Sectors', 'Market Cap']                                                   # Assigns names to the columns of the DataFrame.\n",
    "\n",
    "def format_market_cap(market_cap):                                                                                      # Defines a function to format market cap values.\n",
    "        try:\n",
    "            market_cap = float(market_cap.replace('$', '').replace('B', 'e9').replace('M', 'e6').replace('T', 'e12'))   # Converts the market cap value to a floating-point number.\n",
    "            if market_cap >= 1e12:\n",
    "                return \"${:.2f} T\".format(market_cap / 1e12)                                                            # The function then checks the magnitude of the market cap and appends 'T', 'B', or 'M' accordingly.\n",
    "            elif market_cap >= 1e9:\n",
    "                return \"${:.2f} B\".format(market_cap / 1e9)\n",
    "            else:\n",
    "                return \"${:.2f} M\".format(market_cap / 1e6)\n",
    "            \n",
    "        except ValueError:\n",
    "            return market_cap\n",
    "\n",
    "Sp500_columns[3] = Sp500_columns[3].apply(format_market_cap)     \n",
    "\n",
    "print(\"Resulting DataFrame:\")\n",
    "print(Sp500_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Merging (JOIN) Drop and Rename "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Merged_Sp500_columns = pd.merge(Sp500_data_table_columns, Sp500_columns[[0,3]], left_on='Tickers', right_on=0, how='left')             # pd.merge is used to merge two DataFrames 'JOIN'\n",
    "\n",
    "Merged_Sp500_columns.drop(0, axis=1, inplace=True)                                                                                     # Dropping Unnecessary Columns in merged DataFrames\n",
    "Merged_Sp500_columns.drop('Location', axis=1, inplace=True)\n",
    "Merged_Sp500_columns.drop('Industry', axis=1, inplace=True)\n",
    "\n",
    "Merged_Sp500_columns.rename(columns={ 3: 'Market_Cap'}, inplace=True)                                                                   # Renaming Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Mapping Updates Columns within DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "market_cap_mapping = {\n",
    "    'ABNB': '$85.698B',\n",
    "    'BRK-B': '$773.209B',                                                                                                   # 'Mapping' keys are tickers, and values are the corresponding market cap values \n",
    "    'BX': '$139.435B',\n",
    "    'BF-B': '$29.172B',\n",
    "    'COR': '$40.92B',\n",
    "    'HUBB': '$16.077B',\n",
    "    'KVUE': '$38.623B',\n",
    "    'LULU': '$57.82B',\n",
    "    'VLTO': '$18.609B'}\n",
    "\n",
    "for ticker, market_cap_value in market_cap_mapping.items():                                                                 \n",
    "    Merged_Sp500_columns.loc[Merged_Sp500_columns['Tickers'] == ticker, 'Market_Cap'] = market_cap_value                     # Locates rows in the DataFrame  where 'Tickers' match the current `ticker`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Function Creation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_monthly_average(ticker):                                                                               # Defines a function to calculate the Daily Volume\n",
    "    ticker_monthly_data = yf.download(ticker, period=\"1y\", interval=\"1d\")\n",
    "    monthly_average = ticker_monthly_data['Volume'].resample('M').mean()                                                # Calculates the mean of the Daily Volume for each ticker\n",
    "    return monthly_average.mean()  \n",
    "\n",
    "Merged_Sp500_columns['Average_Monthly_Volume'] = Merged_Sp500_columns['Tickers'].apply(calculate_monthly_average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_weekly_average(ticker):                                                                               # Defines a function to calculate the weekly average\n",
    "    ticker_data = yf.download(ticker, period=\"1y\", interval=\"1d\")\n",
    "    weekly_average = ticker_data['Volume'].resample('W').mean()                                                     # Calculates the mean of the weekly average volumes for each ticker\n",
    "    return weekly_average.mean()  \n",
    "\n",
    "Merged_Sp500_columns['Average_Weekly_Volume'] = Merged_Sp500_columns['Tickers'].apply(calculate_weekly_average)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Formatting (Volume) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locale.setlocale(locale.LC_ALL, '')                                                                                # Using 'locale' to  formatting the 'Daily Volume' column in a DataFrame by adding commas as thousand separators to make it more readable.\n",
    "\n",
    "Merged_Sp500_columns['Average_Monthly_Volume'] = Merged_Sp500_columns['Average_Monthly_Volume'].apply(lambda x: locale.format_string(\"%d\", x, grouping=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locale.setlocale(locale.LC_ALL, '')                                                                                # Using 'locale' to  formatting the 'Weekly Average Volume' column in a DataFrame by adding commas as thousand separators to make it more readable.\n",
    "\n",
    "Merged_Sp500_columns['Average_Weekly_Volume'] = Merged_Sp500_columns['Average_Weekly_Volume'].apply(lambda x: locale.format_string(\"%d\", x, grouping=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Merged_Sp500_columns['Average_Weekly_Volume'] = Merged_Sp500_columns['Average_Weekly_Volume'].astype(int)\n",
    "\n",
    "Merged_Sp500_columns['Average_Monthly_Volume'] = Merged_Sp500_columns['Average_Monthly_Volume'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Merged_Sp500_columns.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Merged_Sp500_columns.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Resulting S&P 500 DataFrame:\")\n",
    "\n",
    "Merged_Sp500_columns.sort_values(by='Stocks',ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exporting DataFrame to SQLite "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primary key added to 'Tickers' column.\n"
     ]
    }
   ],
   "source": [
    "conn = sqlite3.connect('SP500_Database.db')\n",
    "\n",
    "try:\n",
    "    Merged_Sp500_columns.to_sql('SP500_Columns_VL', conn, index=False, if_exists='replace')\n",
    "\n",
    "\n",
    "    # Add the primary key \n",
    "    conn.execute(\"CREATE UNIQUE INDEX idx_unique_Tickers ON SP500_Columns_VL(Tickers)\")\n",
    "\n",
    "    print(\"Primary key added to 'Tickers' column.\")\n",
    "\n",
    "except sqlite3.Error as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "\n",
    "conn.commit()\n",
    "\n",
    "\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exporting DataFrame to Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Merged_Sp500_columns.to_excel('SP500_excel_file.xlsx', index=False) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
